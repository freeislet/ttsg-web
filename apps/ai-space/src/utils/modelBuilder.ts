import * as tf from '@tensorflow/tfjs'
import { createNNModel } from '@/models'
import { LayerNodeData } from '@/types/LayerEditor'
import { 
  convertLayerEditorDataToModelConfig,
  inferLossFunction,
  inferMetrics
} from './layerConfigConverter'

/**
 * ë ˆì´ì–´ ì—ë””í„° ë°ì´í„°ë¡œë¶€í„° TensorFlow.js ëª¨ë¸ ìƒì„±
 */
export async function createModelFromLayerEditor(
  layerNodes: LayerNodeData[],
  inputShape: number[],
  outputUnits: number,
  datasetType?: string
): Promise<tf.Sequential> {
  try {
    // ë ˆì´ì–´ ì—ë””í„° ë°ì´í„°ë¥¼ ëª¨ë¸ ì„¤ì •ìœ¼ë¡œ ë³€í™˜
    const modelConfig = convertLayerEditorDataToModelConfig(
      layerNodes,
      inputShape,
      outputUnits
    )

    console.log('ğŸ”§ Creating model with config:', modelConfig)

    // NNModel ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    const nnModel = createNNModel(modelConfig)
    
    // TensorFlow.js ëª¨ë¸ ìƒì„±
    const model = nnModel.createTFModel()
    
    console.log('âœ… Model created successfully')
    console.log('ğŸ“Š Model summary:')
    model.summary()
    
    return model
  } catch (error) {
    console.error('âŒ Error creating model:', error)
    throw error
  }
}

/**
 * ëª¨ë¸ ì»´íŒŒì¼ (í•™ìŠµ ì¤€ë¹„)
 */
export function compileModel(
  model: tf.Sequential,
  outputUnits: number,
  datasetType?: string,
  customConfig?: {
    optimizer?: string
    learningRate?: number
    loss?: string
    metrics?: string[]
  }
): tf.Sequential {
  try {
    const config = {
      optimizer: customConfig?.optimizer || 'adam',
      learningRate: customConfig?.learningRate || 0.001,
      loss: customConfig?.loss || inferLossFunction(outputUnits, datasetType),
      metrics: customConfig?.metrics || inferMetrics(outputUnits, datasetType)
    }

    console.log('ğŸ”§ Compiling model with config:', config)

    // ì˜µí‹°ë§ˆì´ì € ìƒì„±
    let optimizer: tf.Optimizer
    switch (config.optimizer) {
      case 'sgd':
        optimizer = tf.train.sgd(config.learningRate)
        break
      case 'rmsprop':
        optimizer = tf.train.rmsprop(config.learningRate)
        break
      case 'adam':
      default:
        optimizer = tf.train.adam(config.learningRate)
        break
    }

    // ëª¨ë¸ ì»´íŒŒì¼
    model.compile({
      optimizer,
      loss: config.loss,
      metrics: config.metrics
    })

    console.log('âœ… Model compiled successfully')
    return model
  } catch (error) {
    console.error('âŒ Error compiling model:', error)
    throw error
  }
}

/**
 * ëª¨ë¸ í•™ìŠµ
 */
export async function trainModel(
  model: tf.Sequential,
  trainData: tf.Tensor,
  trainLabels: tf.Tensor,
  config?: {
    epochs?: number
    batchSize?: number
    validationSplit?: number
    callbacks?: tf.CustomCallback[]
  }
): Promise<tf.History> {
  try {
    const trainingConfig = {
      epochs: config?.epochs || 10,
      batchSize: config?.batchSize || 32,
      validationSplit: config?.validationSplit || 0.2,
      callbacks: config?.callbacks || []
    }

    console.log('ğŸš€ Starting model training with config:', trainingConfig)

    const history = await model.fit(trainData, trainLabels, {
      epochs: trainingConfig.epochs,
      batchSize: trainingConfig.batchSize,
      validationSplit: trainingConfig.validationSplit,
      callbacks: trainingConfig.callbacks,
      verbose: 1
    })

    console.log('âœ… Model training completed')
    return history
  } catch (error) {
    console.error('âŒ Error training model:', error)
    throw error
  }
}

/**
 * ëª¨ë¸ ì˜ˆì¸¡
 */
export function predictWithModel(
  model: tf.Sequential,
  inputData: tf.Tensor
): tf.Tensor {
  try {
    console.log('ğŸ”® Making predictions...')
    const predictions = model.predict(inputData) as tf.Tensor
    console.log('âœ… Predictions completed')
    return predictions
  } catch (error) {
    console.error('âŒ Error making predictions:', error)
    throw error
  }
}

/**
 * ëª¨ë¸ í‰ê°€
 */
export async function evaluateModel(
  model: tf.Sequential,
  testData: tf.Tensor,
  testLabels: tf.Tensor
): Promise<tf.Scalar[]> {
  try {
    console.log('ğŸ“Š Evaluating model...')
    const evaluation = await model.evaluate(testData, testLabels) as tf.Scalar[]
    console.log('âœ… Model evaluation completed')
    return evaluation
  } catch (error) {
    console.error('âŒ Error evaluating model:', error)
    throw error
  }
}

/**
 * ì™„ì „í•œ ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
 */
export async function executeModelPipeline(
  layerNodes: LayerNodeData[],
  inputShape: number[],
  outputUnits: number,
  trainData: tf.Tensor,
  trainLabels: tf.Tensor,
  options?: {
    datasetType?: string
    compileConfig?: {
      optimizer?: string
      learningRate?: number
      loss?: string
      metrics?: string[]
    }
    trainingConfig?: {
      epochs?: number
      batchSize?: number
      validationSplit?: number
    }
    testData?: tf.Tensor
    testLabels?: tf.Tensor
  }
): Promise<{
  model: tf.Sequential
  history: tf.History
  evaluation?: tf.Scalar[]
}> {
  try {
    console.log('ğŸš€ Starting complete model pipeline...')

    // 1. ëª¨ë¸ ìƒì„±
    const model = await createModelFromLayerEditor(
      layerNodes,
      inputShape,
      outputUnits,
      options?.datasetType
    )

    // 2. ëª¨ë¸ ì»´íŒŒì¼
    compileModel(model, outputUnits, options?.datasetType, options?.compileConfig)

    // 3. ëª¨ë¸ í•™ìŠµ
    const history = await trainModel(model, trainData, trainLabels, options?.trainingConfig)

    // 4. ëª¨ë¸ í‰ê°€ (í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
    let evaluation: tf.Scalar[] | undefined
    if (options?.testData && options?.testLabels) {
      evaluation = await evaluateModel(model, options.testData, options.testLabels)
    }

    console.log('ğŸ‰ Model pipeline completed successfully!')

    return {
      model,
      history,
      evaluation
    }
  } catch (error) {
    console.error('âŒ Error in model pipeline:', error)
    throw error
  }
}

/**
 * ëª¨ë¸ ë©”ëª¨ë¦¬ ì •ë¦¬
 */
export function disposeModel(model: tf.Sequential): void {
  try {
    model.dispose()
    console.log('ğŸ—‘ï¸ Model disposed successfully')
  } catch (error) {
    console.error('âŒ Error disposing model:', error)
  }
}
